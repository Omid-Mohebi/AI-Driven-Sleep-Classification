{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "gVNd7vtpRgwi",
    "outputId": "57d31052-794f-43b4-f072-f7c02141bbfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Y2cTYR_t_10NAbznspE5bBjuATPdTgtq\n",
      "From (redirected): https://drive.google.com/uc?id=1Y2cTYR_t_10NAbznspE5bBjuATPdTgtq&confirm=t&uuid=1fd25d11-f83e-46c9-97fa-daaeb324729e\n",
      "To: /content/file.zip\n",
      "100%|██████████| 134M/134M [00:04<00:00, 30.5MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'file.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "file_id = '1Y2cTYR_t_10NAbznspE5bBjuATPdTgtq'\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "gdown.download(download_url, 'file.zip', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yA2ZBHoOSS5w"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('file.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsXxHxuRT3C2",
    "outputId": "40ef29b2-6792-4bc1-ebbd-de72eeb05970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1755 samples from class: 2\n",
      "Loaded 497 samples from class: 3\n",
      "Loaded 237 samples from class: 1\n",
      "Loaded 418 samples from class: 4\n",
      "Loaded 2906 samples from class: 0\n",
      "Original class distribution: Counter({0: 2906, 2: 1755, 3: 497, 4: 418, 1: 237})\n",
      "Maximum samples in any class: 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "data_dir = 'data/AISContest_Data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for class_label in os.listdir(data_dir):\n",
    "    class_folder = os.path.join(data_dir, class_label)\n",
    "\n",
    "    if os.path.isdir(class_folder):\n",
    "        class_count = 0\n",
    "\n",
    "        for file_name in os.listdir(class_folder):\n",
    "            if file_name.endswith('.npy'):\n",
    "                file_path = os.path.join(class_folder, file_name)\n",
    "                class_data = np.load(file_path)\n",
    "\n",
    "                data.append(class_data)\n",
    "                labels.append(class_label)\n",
    "\n",
    "                class_count += 1\n",
    "\n",
    "        print(f'Loaded {class_count} samples from class: {class_label}')\n",
    "\n",
    "data = np.array(data, dtype=np.float64)\n",
    "labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "class_counts = Counter(encoded_labels)\n",
    "print(\"Original class distribution:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8l86GXyeUQ6b",
    "outputId": "20e7b5f0-e480-40e1-a72b-f16aca360916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000})\n"
     ]
    }
   ],
   "source": [
    "balanced_data = []\n",
    "balanced_labels = []\n",
    "\n",
    "target_samples = 1000\n",
    "\n",
    "for class_label in np.unique(encoded_labels):\n",
    "    class_indices = np.where(encoded_labels == class_label)[0]\n",
    "\n",
    "    num_samples = len(class_indices)\n",
    "\n",
    "    if num_samples > target_samples:\n",
    "        selected_indices = np.random.choice(class_indices, target_samples, replace=False)\n",
    "    else:\n",
    "        selected_indices = np.random.choice(class_indices, target_samples, replace=True)\n",
    "\n",
    "    balanced_data.append(data[selected_indices])\n",
    "    balanced_labels.append(encoded_labels[selected_indices])\n",
    "\n",
    "balanced_data = np.concatenate(balanced_data)\n",
    "balanced_labels = np.concatenate(balanced_labels)\n",
    "\n",
    "new_class_counts = Counter(balanced_labels)\n",
    "print(new_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMJn8IsoZjRu",
    "outputId": "c1efe334-a9bc-48f1-b656-b0a77d1b7f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (3850, 51, 59, 1)\n",
      "Validation set size: (374, 51, 59, 1)\n",
      "Test set size: (776, 51, 59, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "test_size = 0.23\n",
    "val_size = 0.25\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(balanced_data, balanced_labels, test_size=test_size, random_state=42)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=val_size / (1 - test_size), random_state=42)\n",
    "\n",
    "num_samples, height, width, _ = x_train.shape\n",
    "x_train_reshaped = x_train.reshape(num_samples, height * width)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train_reshaped)\n",
    "\n",
    "x_val_reshaped = x_val.reshape(x_val.shape[0], height * width)\n",
    "x_val_normalized = scaler.transform(x_val_reshaped)\n",
    "\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], height * width)\n",
    "x_test_normalized = scaler.transform(x_test_reshaped)\n",
    "\n",
    "x_train_normalized = x_train_normalized.reshape(x_train.shape)\n",
    "x_val_normalized = x_val_normalized.reshape(x_val.shape)\n",
    "x_test_normalized = x_test_normalized.reshape(x_test.shape)\n",
    "\n",
    "print(f'Training set size: {x_train_normalized.shape}')\n",
    "print(f'Validation set size: {x_val_normalized.shape}')\n",
    "print(f'Test set size: {x_test_normalized.shape}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
