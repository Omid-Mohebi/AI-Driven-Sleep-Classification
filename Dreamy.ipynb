{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "gVNd7vtpRgwi",
    "outputId": "57d31052-794f-43b4-f072-f7c02141bbfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Y2cTYR_t_10NAbznspE5bBjuATPdTgtq\n",
      "From (redirected): https://drive.google.com/uc?id=1Y2cTYR_t_10NAbznspE5bBjuATPdTgtq&confirm=t&uuid=1fd25d11-f83e-46c9-97fa-daaeb324729e\n",
      "To: /content/file.zip\n",
      "100%|██████████| 134M/134M [00:04<00:00, 30.5MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'file.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "file_id = '1Y2cTYR_t_10NAbznspE5bBjuATPdTgtq'\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "gdown.download(download_url, 'file.zip', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yA2ZBHoOSS5w"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('file.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsXxHxuRT3C2",
    "outputId": "40ef29b2-6792-4bc1-ebbd-de72eeb05970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1755 samples from class: 2\n",
      "Loaded 497 samples from class: 3\n",
      "Loaded 237 samples from class: 1\n",
      "Loaded 418 samples from class: 4\n",
      "Loaded 2906 samples from class: 0\n",
      "Original class distribution: Counter({0: 2906, 2: 1755, 3: 497, 4: 418, 1: 237})\n",
      "Maximum samples in any class: 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "data_dir = 'data/AISContest_Data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for class_label in os.listdir(data_dir):\n",
    "    class_folder = os.path.join(data_dir, class_label)\n",
    "\n",
    "    if os.path.isdir(class_folder):\n",
    "        class_count = 0\n",
    "\n",
    "        for file_name in os.listdir(class_folder):\n",
    "            if file_name.endswith('.npy'):\n",
    "                file_path = os.path.join(class_folder, file_name)\n",
    "                class_data = np.load(file_path)\n",
    "\n",
    "                data.append(class_data)\n",
    "                labels.append(class_label)\n",
    "\n",
    "                class_count += 1\n",
    "\n",
    "        print(f'Loaded {class_count} samples from class: {class_label}')\n",
    "\n",
    "data = np.array(data, dtype=np.float64)\n",
    "labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "class_counts = Counter(encoded_labels)\n",
    "print(\"Original class distribution:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8l86GXyeUQ6b",
    "outputId": "20e7b5f0-e480-40e1-a72b-f16aca360916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000})\n"
     ]
    }
   ],
   "source": [
    "balanced_data = []\n",
    "balanced_labels = []\n",
    "\n",
    "target_samples = 1000\n",
    "\n",
    "for class_label in np.unique(encoded_labels):\n",
    "    class_indices = np.where(encoded_labels == class_label)[0]\n",
    "\n",
    "    num_samples = len(class_indices)\n",
    "\n",
    "    if num_samples > target_samples:\n",
    "        selected_indices = np.random.choice(class_indices, target_samples, replace=False)\n",
    "    else:\n",
    "        selected_indices = np.random.choice(class_indices, target_samples, replace=True)\n",
    "\n",
    "    balanced_data.append(data[selected_indices])\n",
    "    balanced_labels.append(encoded_labels[selected_indices])\n",
    "\n",
    "balanced_data = np.concatenate(balanced_data)\n",
    "balanced_labels = np.concatenate(balanced_labels)\n",
    "\n",
    "new_class_counts = Counter(balanced_labels)\n",
    "print(new_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMJn8IsoZjRu",
    "outputId": "c1efe334-a9bc-48f1-b656-b0a77d1b7f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (3850, 51, 59, 1)\n",
      "Validation set size: (374, 51, 59, 1)\n",
      "Test set size: (776, 51, 59, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "test_size = 0.23\n",
    "val_size = 0.25\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(balanced_data, balanced_labels, test_size=test_size, random_state=42)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=val_size / (1 - test_size), random_state=42)\n",
    "\n",
    "num_samples, height, width, _ = x_train.shape\n",
    "x_train_reshaped = x_train.reshape(num_samples, height * width)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train_reshaped)\n",
    "\n",
    "x_val_reshaped = x_val.reshape(x_val.shape[0], height * width)\n",
    "x_val_normalized = scaler.transform(x_val_reshaped)\n",
    "\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], height * width)\n",
    "x_test_normalized = scaler.transform(x_test_reshaped)\n",
    "\n",
    "x_train_normalized = x_train_normalized.reshape(x_train.shape)\n",
    "x_val_normalized = x_val_normalized.reshape(x_val.shape)\n",
    "x_test_normalized = x_test_normalized.reshape(x_test.shape)\n",
    "\n",
    "print(f'Training set size: {x_train_normalized.shape}')\n",
    "print(f'Validation set size: {x_val_normalized.shape}')\n",
    "print(f'Test set size: {x_test_normalized.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ny5FYyYb2QL",
    "outputId": "b9caca37-685b-4eb3-ffe2-a661f81c1fc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, input_shape=(51,59,1), kernel_initializer='glorot_uniform'))\n",
    "model.add(Conv2D(32, kernel_size=3, padding='same', kernel_initializer='glorot_uniform'))\n",
    "model.add(Conv2D(32, kernel_size=3, padding='same', kernel_initializer='glorot_uniform'))\n",
    "model.add(Conv2D(32, kernel_size=3, padding='same', kernel_initializer='glorot_uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Conv2D(64, kernel_size=3, padding='same', kernel_initializer='glorot_uniform'))\n",
    "model.add(Conv2D(64, kernel_size=3, padding='same', kernel_initializer='glorot_uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', kernel_initializer='glorot_uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dense(len(set(labels)), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "8pNRtNLqci6d"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qc7ymjsddGuK",
    "outputId": "7e079d53-2275-4b33-af59-595377de1aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 3s/step - accuracy: 0.2054 - loss: 1.6272 - val_accuracy: 0.2299 - val_loss: 1.6667\n",
      "Epoch 2/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 3s/step - accuracy: 0.2361 - loss: 1.6105 - val_accuracy: 0.2273 - val_loss: 1.6127\n",
      "Epoch 3/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 3s/step - accuracy: 0.2859 - loss: 1.5422 - val_accuracy: 0.2406 - val_loss: 1.6271\n",
      "Epoch 4/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3s/step - accuracy: 0.3870 - loss: 1.3948 - val_accuracy: 0.4679 - val_loss: 1.4515\n",
      "Epoch 5/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 3s/step - accuracy: 0.5426 - loss: 1.1352 - val_accuracy: 0.5160 - val_loss: 1.4998\n",
      "Epoch 6/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 3s/step - accuracy: 0.6399 - loss: 0.9283 - val_accuracy: 0.5909 - val_loss: 1.4353\n",
      "Epoch 7/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 3s/step - accuracy: 0.7489 - loss: 0.6668 - val_accuracy: 0.6203 - val_loss: 1.6668\n",
      "Epoch 8/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 3s/step - accuracy: 0.8352 - loss: 0.4593 - val_accuracy: 0.5909 - val_loss: 2.5289\n",
      "Epoch 9/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 3s/step - accuracy: 0.8090 - loss: 0.5207 - val_accuracy: 0.6390 - val_loss: 1.7278\n",
      "Epoch 10/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 3s/step - accuracy: 0.8392 - loss: 0.4486 - val_accuracy: 0.6337 - val_loss: 1.9838\n",
      "Epoch 11/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 3s/step - accuracy: 0.8774 - loss: 0.3370 - val_accuracy: 0.6578 - val_loss: 1.9818\n",
      "Epoch 12/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 3s/step - accuracy: 0.8798 - loss: 0.3444 - val_accuracy: 0.6872 - val_loss: 2.2226\n",
      "Epoch 13/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 3s/step - accuracy: 0.9090 - loss: 0.2298 - val_accuracy: 0.6604 - val_loss: 2.7143\n",
      "Epoch 14/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4s/step - accuracy: 0.9305 - loss: 0.2079 - val_accuracy: 0.6898 - val_loss: 2.6034\n",
      "Epoch 15/15\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 3s/step - accuracy: 0.9429 - loss: 0.1657 - val_accuracy: 0.6578 - val_loss: 2.8245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=len(set(labels)))\n",
    "y_val = to_categorical(y_val, num_classes=len(set(labels)))\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(x_train_normalized, y_train,\n",
    "                    validation_data=(x_val_normalized, y_val),\n",
    "                    epochs=15,\n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSgBY9MRfkAM",
    "outputId": "ba625acf-2acc-4b05-afc3-187403229502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step\n",
      "F1 Micro Score: 0.6843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test_normalized)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "f1_micro = f1_score(y_test, y_pred_classes, average='micro')\n",
    "print(f'F1 Micro Score: {f1_micro:.4f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
